#!/usr/bin/env python3
"""
Enhanced Batch Executive Extractor
Processes companies from JSON (chat agent) or CSV files and extracts executive information.
"""

import sys
import json
import time
import argparse
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import pandas as pd

from serpapi_searcher import SerpAPISearcher
from content_scraper import ContentScraper
from executive_extractor import ExecutiveExtractor
from data_exporter import DataExporter
from data_loader import DataLoader
from config import BATCH_CONFIG, CXO_POSITIONS

class BatchExtractor:
    def __init__(self):
        self.serpapi_searcher = SerpAPISearcher()
        self.content_scraper = ContentScraper()
        self.executive_extractor = ExecutiveExtractor()
        self.data_exporter = DataExporter()
        self.data_loader = DataLoader()
        
        # Setup logging
        self.setup_logging()
        
        # Load progress tracking
        self.progress_file = BATCH_CONFIG['progress_file']
        self.progress_data = self.load_progress()
    
    def setup_logging(self):
        """Setup logging configuration"""
        log_file = BATCH_CONFIG['log_file']
        log_level = getattr(logging, BATCH_CONFIG['log_level'].upper())
        
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_progress(self) -> Dict[str, Any]:
        """Load progress tracking data"""
        try:
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {
                'last_processed_company': None,
                'processed_companies': [],
                'last_processing_date': None,
                'total_companies_processed': 0,
                'total_executives_found': 0
            }
    
    def save_progress(self):
        """Save progress tracking data"""
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.progress_data, f, indent=2, ensure_ascii=False)
    
    def load_companies_from_csv(self) -> List[Dict[str, Any]]:
        """Load companies from CSV file (legacy method for backward compatibility)"""
        try:
            data = self.data_loader.load_from_csv(BATCH_CONFIG['companies_csv_file'])
            
            # Filter by UAE companies if country filter is set
            if BATCH_CONFIG.get('country_filter'):
                companies = [c for c in data['companies'] if c['country'] == BATCH_CONFIG['country_filter']]
                self.logger.info(f"Loaded {len(companies)} {BATCH_CONFIG['country_filter']} companies from CSV")
            else:
                companies = data['companies']
                self.logger.info(f"Loaded {len(companies)} companies from CSV")
            
            return companies
            
        except Exception as e:
            self.logger.error(f"Error loading companies from CSV: {e}")
            return []
    
    def load_companies_from_json(self, json_data: str) -> List[Dict[str, Any]]:
        """Load companies from JSON string (generated by chat agent)"""
        try:
            data = self.data_loader.load_from_json(json_data)
            companies = data['companies']
            self.logger.info(f"Loaded {len(companies)} companies from JSON")
            return companies
            
        except Exception as e:
            self.logger.error(f"Error loading companies from JSON: {e}")
            return []
    
    def load_companies(self, source: str = 'csv', json_data: str = None) -> List[Dict[str, Any]]:
        """Load companies from specified source"""
        if source == 'json' and json_data:
            return self.load_companies_from_json(json_data)
        else:
            return self.load_companies_from_csv()
    
    def filter_recent_companies(self, companies: List[Dict[str, Any]], days: int) -> List[Dict[str, Any]]:
        """Filter companies that were recently added (based on progress data)"""
        if not self.progress_data['last_processing_date']:
            return companies
        
        last_date = datetime.strptime(self.progress_data['last_processing_date'], '%Y-%m-%d')
        threshold_date = last_date - timedelta(days=days)
        
        # For now, return all companies since we don't track when companies were added
        # In a real implementation, you might want to track this in the CSV or progress file
        self.logger.info(f"Processing all companies (recent filtering not implemented)")
        return companies
    
    def filter_specific_companies(self, companies: List[Dict[str, Any]], company_names: List[str]) -> List[Dict[str, Any]]:
        """Filter specific companies by name"""
        filtered = []
        for company in companies:
            if company['name'] in company_names:
                filtered.append(company)
        
        self.logger.info(f"Filtered to {len(filtered)} specific companies")
        return filtered
    
    def generate_company_queries(self, company: Dict[str, Any]) -> List[str]:
        """Generate optimized search queries for a company using LLM"""
        company_name = company['name']
        city = company['city']
        industry = company['industry']
        
        try:
            # Use OpenAI to generate optimized queries
            if (BATCH_CONFIG.get('llm_query_generation', True) and 
                hasattr(self.executive_extractor, 'client') and 
                self.executive_extractor.client):
                prompt = f"""
                Generate 1-2 highly effective Google search queries to find CURRENT executive information for this company.
                
                Company: {company_name}
                Location: {city}
                Industry: {industry}
                
                Focus on finding CURRENT (not historical) CXO-level executives:
                - CEO, CFO, CMO, CTO, COO, CIO, and other C-level executives
                - Board members and senior management
                - Their LinkedIn profiles and email addresses
                
                Generate queries that are:
                1. Highly specific and targeted for CURRENT executives
                2. Likely to return executive information
                3. Include "current", "executive team", "LinkedIn" for better results
                4. Focus on the company name and location
                
                Return only the search queries, one per line, without numbering or explanations.
                Example format:
                {company_name} current executive team linkedin and email addresses
                {company_name} board of directors current members linkedin
                """
                
                response = self.executive_extractor.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=200,
                    temperature=0.3
                )
                
                content = response.choices[0].message.content.strip()
                
                # Parse the queries
                queries = [line.strip() for line in content.split('\n') if line.strip()]
                
                # Add fallback query if LLM generates too few
                if len(queries) < 1:
                    queries.append(f"{company_name} current executive team linkedin and email addresses")
                
                # Limit to maximum 2 queries
                queries = queries[:2]
                
                self.logger.info(f"Generated {len(queries)} optimized queries for {company_name}")
                return queries
                
            else:
                # Fallback to basic queries if OpenAI is not available
                return self._generate_fallback_queries(company)
                
        except Exception as e:
            self.logger.warning(f"LLM query generation failed for {company_name}: {e}")
            return self._generate_fallback_queries(company)
    
    def _generate_fallback_queries(self, company: Dict[str, Any]) -> List[str]:
        """Fallback query generation when LLM is not available"""
        company_name = company['name']
        city = company['city']
        
        # Generate 1-2 focused queries for current executives
        queries = [
            f"{company_name} current executive team linkedin and email addresses"
        ]
        
        # Add a second query only if needed
        if len(queries) < 2:
            queries.append(f"{company_name} board of directors current members linkedin")
        
        self.logger.info(f"Generated {len(queries)} fallback queries for {company_name}")
        return queries
    
    def process_single_company(self, company: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Process a single company and extract executives with smart optimization"""
        company_name = company['name']
        self.logger.info(f"Processing company: {company_name}")
        
        try:
            # Generate queries for this company
            queries = self.generate_company_queries(company)
            self.logger.info(f"Generated {len(queries)} queries for {company_name}")
            
            # Track found executives to avoid duplicates
            found_executives = set()
            all_executives = []
            target_executive_count = BATCH_CONFIG.get('target_executives_per_company', 5)
            
            # Collect all articles first
            all_articles = []
            
            for query_idx, query in enumerate(queries):
                if (BATCH_CONFIG.get('enable_early_termination', True) and 
                    len(all_executives) >= target_executive_count):
                    self.logger.info(f"✅ Found sufficient executives ({len(all_executives)}) for {company_name}, stopping early")
                    break
                
                self.logger.info(f"Searching query {query_idx + 1}/{len(queries)}: {query}")
                
                # Search for content with this query
                search_results = self.serpapi_searcher.search_google(
                    query, 
                    max_results=BATCH_CONFIG.get('max_results_per_query', 5)
                )
                
                if not search_results:
                    continue
                
                self.logger.info(f"Found {len(search_results)} search results for query: {query}")
                
                # Process search results with early termination
                for result_idx, result in enumerate(search_results):
                    if len(all_executives) >= target_executive_count:
                        self.logger.info(f"✅ Reached target executive count, stopping at result {result_idx + 1}")
                        break
                    
                    try:
                        # Scrape single article
                        article = self.content_scraper.process_single_article(result['url'])
                        if not article:
                            continue
                        
                        # Add search metadata
                        article.update({
                            'search_title': result['title'],
                            'search_snippet': result['snippet'],
                            'search_query': query
                        })
                        
                        all_articles.append(article)
                        
                    except Exception as e:
                        self.logger.warning(f"Error processing result {result_idx + 1}: {e}")
                        continue
                
                # Delay between queries
                if query_idx < len(queries) - 1:
                    delay = BATCH_CONFIG['delay_between_queries']
                    time.sleep(delay)
            
            # Extract executives from all articles with enrichment
            if all_articles:
                self.logger.info(f"Processing {len(all_articles)} articles for executive extraction and enrichment...")
                all_executives = self.executive_extractor.extract_executives_from_articles(all_articles)
                
                # Add company metadata to all executives
                for executive in all_executives:
                    executive['company_industry'] = company['industry']
                    # Use LLM-based company name matching if enabled
                    if BATCH_CONFIG['llm_company_matching']:
                        executive['bank'] = self.match_company_name(executive.get('bank', ''), company_name)
                    else:
                        executive['bank'] = company_name
            
            self.logger.info(f"Extracted {len(all_executives)} unique executives for {company_name}")
            return all_executives
            
        except Exception as e:
            self.logger.error(f"Error processing company {company_name}: {e}")
            return []
    
    def match_company_name(self, extracted_name: str, actual_name: str) -> str:
        """Use LLM to match extracted company name with actual company name"""
        if not extracted_name or extracted_name.lower() == actual_name.lower():
            return actual_name
        
        try:
            # Use OpenAI to determine if the extracted name matches the actual company
            if hasattr(self.executive_extractor, 'client') and self.executive_extractor.client:
                prompt = f"""
                Determine if these two company names refer to the same company:
                
                Extracted name: "{extracted_name}"
                Actual company name: "{actual_name}"
                
                Return only "YES" if they are the same company, or "NO" if they are different.
                Consider abbreviations, common variations, and parent/subsidiary relationships.
                """
                
                response = self.executive_extractor.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=10,
                    temperature=0.1
                )
                
                result = response.choices[0].message.content.strip().upper()
                if result == "YES":
                    return actual_name
                else:
                    return extracted_name
            else:
                # Fallback to simple matching
                if extracted_name.lower() in actual_name.lower() or actual_name.lower() in extracted_name.lower():
                    return actual_name
                return extracted_name
                
        except Exception as e:
            self.logger.warning(f"LLM company matching failed: {e}")
            return actual_name
    
    def run(self, recent_days: Optional[int] = None, specific_companies: Optional[List[str]] = None, resume: bool = False, 
            source: str = 'csv', json_data: str = None):
        """Main batch processing function"""
        self.logger.info("🚀 Starting Enhanced Batch Executive Extractor")
        self.logger.info("="*60)
        
        # Load companies from specified source
        all_companies = self.load_companies(source=source, json_data=json_data)
        if not all_companies:
            if source == 'json':
                self.logger.error("No companies found in JSON data")
            else:
                self.logger.error("No companies found in CSV file")
            return
        
        # Apply filters
        companies_to_process = all_companies
        
        if specific_companies:
            companies_to_process = self.filter_specific_companies(all_companies, specific_companies)
        elif recent_days:
            companies_to_process = self.filter_recent_companies(all_companies, recent_days)
        
        if resume and self.progress_data['last_processed_company']:
            # Find the index of the last processed company
            last_company = self.progress_data['last_processed_company']
            try:
                last_index = next(i for i, c in enumerate(companies_to_process) if c['name'] == last_company)
                companies_to_process = companies_to_process[last_index + 1:]
                self.logger.info(f"Resuming from company after: {last_company}")
            except StopIteration:
                self.logger.info("Last processed company not found in current list, processing all")
        
        if not companies_to_process:
            self.logger.info("No companies to process")
            return
        
        self.logger.info(f"Processing {len(companies_to_process)} companies")
        
        # Process companies
        all_executives = []
        processed_count = 0
        
        for i, company in enumerate(companies_to_process):
            try:
                self.logger.info(f"\n📊 Progress: {i+1}/{len(companies_to_process)} - {company['name']}")
                
                # Process company
                company_executives = self.process_single_company(company)
                all_executives.extend(company_executives)
                
                # Update progress
                self.progress_data['last_processed_company'] = company['name']
                self.progress_data['processed_companies'].append(company['name'])
                self.progress_data['last_processing_date'] = datetime.now().strftime('%Y-%m-%d')
                self.progress_data['total_companies_processed'] += 1
                self.progress_data['total_executives_found'] += len(company_executives)
                
                processed_count += 1
                
                # Save progress after each company
                self.save_progress()
                
                # Delay between companies
                if i < len(companies_to_process) - 1:
                    delay = BATCH_CONFIG['delay_between_companies']
                    self.logger.info(f"Waiting {delay} seconds before next company...")
                    time.sleep(delay)
                
            except KeyboardInterrupt:
                self.logger.info("\n⏹️ Batch processing interrupted by user")
                break
            except Exception as e:
                self.logger.error(f"Error processing company {company['name']}: {e}")
                continue
        
        # Export results
        if all_executives:
            self.logger.info(f"\n💾 Exporting {len(all_executives)} executives...")
            
            # Append to existing CSV files
            csv_file = self.data_exporter.export_to_csv(all_executives, append_mode=True, batch_mode=True)
            detailed_csv = self.data_exporter.export_detailed_csv(all_executives, append_mode=True, batch_mode=True)
            
            # Generate summary
            summary = self.data_exporter.generate_summary_report(all_executives)
            self.data_exporter.print_summary_report(summary)
            
            self.logger.info(f"\n🎉 Batch processing completed!")
            self.logger.info(f"📁 Files updated:")
            self.logger.info(f"  - {csv_file}")
            self.logger.info(f"  - {detailed_csv}")
        else:
            self.logger.warning("No executives found during batch processing")

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='Enhanced Batch Executive Extractor')
    parser.add_argument('--recent', type=int, help='Process only companies added in the last N days')
    parser.add_argument('--companies', nargs='+', help='Process specific companies by name')
    parser.add_argument('--resume', action='store_true', help='Resume from last processed company')
    parser.add_argument('--source', choices=['csv', 'json'], default='csv', help='Data source type (csv or json)')
    parser.add_argument('--json-data', type=str, help='JSON data string (required when source=json)')
    
    args = parser.parse_args()
    
    # Validate arguments
    if args.source == 'json' and not args.json_data:
        print("Error: --json-data is required when --source=json")
        return
    
    extractor = BatchExtractor()
    extractor.run(
        recent_days=args.recent,
        specific_companies=args.companies,
        resume=args.resume,
        source=args.source,
        json_data=args.json_data
    )

if __name__ == "__main__":
    main() 